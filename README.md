# Detection-and-recognition-of-driver-distraction-using-multimodal-signals

Distracted driving is a leading cause of accidents worldwide. The tasks of distraction detection and recognition have been traditionally addressed as computer vision problems. However, distracted behaviors are
not always expressed in a visually observable way. In this work, we introduce a novel multimodal dataset of distracted driver behaviors, consisting of data collected using twelve information channels coming from
visual, acoustic, near-infrared, thermal, physiological and linguistic modalities. The data were collected from 45 subjects while being exposed to four different distractions (three cognitive and one physical). For the purposes
of this paper, we performed experiments with visual, physiological, and thermal information to explore potential of multimodal modeling for distraction recognition. In addition, we analyze the value of different
modalities by identifying specific visual, physiological, and thermal groups of features that contribute the most to distraction characterization. Our results highlight the advantage of multimodal representations and
reveal valuable insights for the role played by the three modalities on identifying different types of driving distractions.

CCS Concepts: • Human-centered computing→Empirical studies in ubiquitous and mobile computing;
• Social and professional topics → User characteristics; • Information systems → Multimedia and multimodal retrieval;

Additional Key Words and Phrases: Distracted driving, machine learning, physiological signal processing, action unit analysis, thermal (keyword), multimodal interaction, multimodal datasets
